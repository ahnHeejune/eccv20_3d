\section{Transfer of 3D cloth model to the target Human and Virtual Try-On }  \label{section:clothtransfer}


\subsection{Transfer of 3D cloth model to the target Human} 

The 3D model and texture information obtained from 3D reconstruction (Section \ref{section:3dclothrecon}) are for the standard shape and posed person. To apply this information for virtual try-on application, we have to apply the shape and pose parameters of the target human image estimated from SMPLify\cite{Bogo2016SMPLify} step.  Instead of applying the shape and pose parameters to the obtained clothed 3D model, we transfer the displacement of cloth vertices to the target human body model, because the application of new parameters to the Body model provide much better natural results.      


Multiple options can be considered for the transfer. We could transfer the physical size of cloth or keep the fit, i.e., keep the displacement from the body to cloth vertices as before.  We simply decide the fit-preserving option for showing more natural results for final fitting.  

Technically, the displacement should be calculated locally. First, we calculated the local coordinate at each vertices. We define the local coordinates: surface normal vector as z-axis, and the vector to smallest indexed edges as x-axis, and their cross product vector as y-axis as the following equations.


\begin{align}
 u_{z} =  normal(V_{body})  \\
 u_{x} = u^{''}_{x}/ |u^{''}_{x} |, 
 u^{''}_{x} = u^{'}_{x} - u^{'}_{x} \cdot u_{z}, 
 u^{'}_{x} = (V_{argmin(N_V) } - V_{body}) \\
 u_{y}  =  u_z \otimes u_x,
\end{align} 
 where $N_V$ is the neighbor vertex of $V$.
 

The displacement is expressed in the local coordinates and then used the same way in the new target body surfaces for location transfer.

\begin{equation}
\overrightarrow{d} = (d_x, d_y, d_z) = V_{clothed} - V_{body} \: in \: (u_x, u_y, u_z | V_{body})
\end{equation}


\begin{equation}
 V'_{clothed} = V'_{body} + \overrightarrow{d} \: in \: (u_x, u_y, u_z | V'_{body})
\end{equation}



\subsection{Blending of warped cloth with target human image}

For our experiment, we used an extended version of Try-on module (TOM) from CP-VTON\cite{Wang2018TowardCI}. We trained  our updated try-on module with the dataset collected by Han et al.\cite{Han2017VITONAI}, and used that trained network for testing with the 3D warped cloth. We updated 3 things from original Try-on module (TOM) of CP-VTON\cite{Wang2018TowardCI} for our implementation. Firstly, we included the un-intended body and clothing areas into the person representation input to Try-on module (TOM) network\cite{Wang2018TowardCI}. Secondly, we included the warped cloth mask into network inputs, so that it can work better for the white-colored clothes. Thirdly, we updated the composite mask loss function\cite{Wang2018TowardCI}.


This part is under implementation.

We first tried to use TOM. But we found when the reconstruction is not perfect the blending is not natural 

Other option is first reconstruct all the clothed information from the target user and overlay the transferred cloth.






