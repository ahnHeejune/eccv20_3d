\section{Introduction}


Online fashion market has been growing rapidly every year. Unlike electronics, which makes it easy to standardize functions and performances, fashion apparels have infinite variations in style, forms, colors, texture, and materials.  Also the difference between personal preferences is huge. As a result, clothing purchasing decisions are very difficult to make with current non-customized information, like the cloth and models' try fit images. Therefore, virtual try-on (VTON) is a highly demanding technology for the online shopping. 

The early VTON technologies were based on 3D computer graphics technology that uses 3D models for a target human and clothing, which are usually expensive and difficult to obtain. Therefore, recently 2D image-based VTON technologies are being studied in academia and industry, fueled by the recent advances in computer vision technologies based on deep learning (DL). 

There have been many assumptions in problem settings from the general conditional human image generation related to VTON application. We consider the one with a try-on cloth and target human image is a practical condition which is assumed in many papers VITON\cite{Han2017VITONAI}, CP-VTON\cite{Wang2018TowardCI} , and the following  \cite{Sun2019ImageBasedVT,Yu_2019_ICCV}. Therefore, we also consider the virtual try-on problem that uses the try-on clothes and human images and generates a new synthetic image that the target human replaced the current top or bottom cloth with the try-on cloth. In this paper, we limit our application to top cloth only due to the restricted data set. However, we consider that the bottom, e.g. pants cases would be easier than top cloth cases.

The existing image based algorithms seemingly generate high quality virtual try-on images. However, our classified analysis on the cloth styles, human poses, and shapes reveals significant problems, illustrated in Figure \ref{fig:classified2DVTONresult}. One reason of the seemingly high quality in the existing algorithms are mainly due to the low complexity of the dataset, i.e., most clothes are short-sleeved, and mono-colored, and the poses of humans are mild. Specifically, the results with the long-sleeved cloth arm and body posed shows far low quality of the presented result in their papers. We identified 5 issues in CP-VTON\cite{Wang2018TowardCI} algorithms, some of which are tackled in the following papers. Firstly, the target try-on area is dependent upon current cloth shape. Especially, the neck area pixels are labeled as background and some body areas are occluded by hairs or accessories (Fig. \ref{fig:classified2DVTONresult} (b) left), which affects in cloth warping and blending. Secondly, all the unintended parts, i.e., face, bottom-clothes and legs need to be preserved in blending stage. However, other body parts except face and hairs are missing in CP-VTON\cite{Wang2018TowardCI} human representation inputs and generated at blending stage, which is all right for general synthesis application, but not desirable in virtual try-on application (Fig. \ref{fig:classified2DVTONresult} (b) left). Thirdly, the texture is often not vivid, which is due to the composition. Examining the original loss function of Try-on Module (TOM) network, the term for the composition alpha mask are poorly formulated as in simple regularization loss.   

\begin{equation}
L = c_1 | I_0-I_{GT} |+  c_2 L_{VGG}+c_3 |1-M_0 |        
\end{equation} 

Fourthly, since no label in the area of warped cloth is the same color as background, white colored clothes are confused and improperly processed in the blending stage (Fig. \ref{fig:classified2DVTONresult} (c))
Finally, Geometric Matching Module (GMM) using Spatial Transform Network\cite{JaderbergSZK15} with TPS (Thin Plate Spline)\cite{Bookstein1989PrincipalWT} deformation cannot handle strong 3D deformation due to the target pose, also  generates artifacts due to the person representation inputs. For example, hands-up and folded arms.  Note that many errors in the warping stage are often hidden in the blending stage, especially when the target clothes are single-colored, which can be expected in practical conditions (Fig. \ref{fig:classified2DVTONresult} (d)).

In this paper, we focus on the last but most difficult problems that cannot be solved in pure 2D image-based algorithm. The 3D cloth deformation is inherently difficult for 2D warping method, including non-rigid one, like TPS algorithm, we propose to first reconstruct 3D model of try-on cloth, then apply the pose and shape transfer for the target human, and finally blending with unchanged image contents like the face, bottom cloth, and background. Therefore, one of main the tasks now is to reconstruct 3D cloth model from 2D try-on cloth image. 3D clothed body model reconstruction have been studied in previous works \cite{natsume2019siclope,saito2019pifu}, however it still needs significant improvements for general condition. Our key idea in this step is that, once we can control the human pose and body shape to become similar to the try-on cloth's, the 3D reconstruction process can be made much easier, and the reconstruction quality would be much higher than general pose and shape condition. 

Therefore, in the Section \ref{section:3dclothrecon}, we describe the 3D cloth reconstruction algorithm. We divide the reconstruction step into 2D matching of cloth to the standard body silhouette and the 3D reconstruction of cloth. The later 3D reconstruction step is done through the SMPLify\cite{Bogo2016SMPLify} algorithm for the SMPL 3D body model\cite{Loper2015SMPLAS}.  In Section \ref{section:clothtransfer}, we describe the blending method, where the 3D cloth models are transferred to the target human images, through SMPL body parameters of shapes and poses. Then the transferred 3D reconstructed cloth is rendered and blended to the target human image. In this step, we reused the 2D VTON blending algorithm with the modification for the condition.  The sampled results from dataset are presented in Section \ref{section:clothtransfer} and the paper is concluded in Section \ref{section:conclusion}. In addition to our main study, we added the classified quality evaluation of the previous 2D image based virtual try-on algorithms for the completeness of the paper.


